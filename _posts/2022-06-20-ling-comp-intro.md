---
layout: post
title: "Le tante facce della linguistica computazionale üß†"
date: 2022-06-20
tag: concetti base
---

Definire cose complesse in maniera semplice non √® mai facile. Quando parliamo di grandi ambiti disciplinari, poi, risulta essere ancora meno facile. La linguistica computazionale √® un ambito effettivamente molto grande: la sua storia non √® lunghissima (spero che ci torneremo!), eppure si √® sviluppata negli anni attraverso modelli teorici e tecnologie sempre diversi, per raggiungere obiettivi sempre diversi, intersecandosi, in maniera pi√π o meno profonda in base al periodo, con aree sempre diverse.

Una definizione sbrigativa e moderatamente funzionale potrebbe essere: *la linguistica computazionale √® quella parte dell'intelligenza artificiale che si occupa del linguaggio*. Una leggermente pi√π tecnica, invece: *la linguistica computazionale √® quell'area che addestra modelli statistico-probabilistici di intelligenza artificiale per dotarli di conoscenza linguistica*. Una pi√π simpatica (la mia preferita): *la linguistica computazionale insegna ai computer a parlare*. Molte altre sarebbero formulabili, e ciascuna andrebbe pi√π o meno bene, ma nessuna potrebbe, da sola, cogliere tutte le tante facce che questa vasta area disciplinare ha.

Piuttosto, per inquadrare meglio la linguistica computazionale, vale la pena soffermarsi su come ci si pu√≤ riferire oggi a questo ambito. **Le espressioni con cui lo si indica sono infatti tante, e ciascuna pone l'accento su un aspetto diverso: insieme, permettono di creare un quadro piuttosto completo degli interessi di ricerca della linguistica computazionale.** Al riguardo, due disclaimer sono doverosi. Il primo: capita che si considerino queste espressioni come relative ad aree effettivamente diverse, per quanto correlate: qui, per comodit√†, preferisco considerarle semplicemente modi diversi di indicare un unico macro-ambito (ciascuno ponendo l'accento, come gi√† detto, su aspetti particolari dello stesso). Il secondo: come ogni area ancora giovane e interdisciplinare che si rispetti, le definizioni possono essere diversissime in base alla particolare fonte e al particolare autore, e ogni confine √® sfumato.


La prima espressione √®, ovviamente, **linguistica computazionale**. √à il nome storico, e il principale, con cui la disciplina √® nota in Italia, e pone chiaramente l'accento sulla componente "linguistica" dell'ambito (creando continuit√†, ad esempio, con espressioni come *linguistica generale*, *testuale*, *del contatto*, ecc.). In effetti, la linguistica computazionale pu√≤ essere intesa come **quell'area che utilizza il computer e l'interazione col computer per meglio comprendere il linguaggio umano**. Un computer √® in grado di lavorare con una quantit√† di dati testuali impressionante, e di compiere in pochissimi istanti operazioni che per noi umani sarebbero estremamente laboriose, lente e noiose (ad esempio, calcolare quante volte compare la parola "treno" in una copia di *Anna Karenina*...). Questo permette di fare analisi statistiche e quantitative importantissime, che possono aiutare a fare luce su fenomeni linguistici ancora oscuri e di esplorare i testi da una prospettiva inedita.

La linguistica computazionale, per√≤, non si limita solo a usare il computer per meglio capire il linguaggio. Si interessa anche a far s√¨ che il computer stesso possa lavorare in maniera autonoma coi testi. Il problema √® che il linguaggio √® un sistema *estremamente* complesso, e la competenza linguistica che abbiamo noi umani √® strabiliante: ogni volta che diciamo qualcosa o leggiamo qualcosa, produciamo o interpretiamo dei testi il cui significato √® sorretto da tantissime caratteristiche diverse (sintattiche, lessicali, semantiche, pragmatiche... e la lista potrebbe andare tranquillamente avanti). Ogni testo √®, non solo etimologicamente, un tessuto. E il computer, nonostante sia in grado di fare calcoli impressionanti, ragiona attraverso unit√† pi√π rigidi di quelle con cui noi produciamo questi tessuti: lo 0 e l'1. Da qui, **la necessit√† di creare formalismi del linguaggio**, di rappresentarlo cio√® attraverso schemi di rappresentazione rigidi e inflessibili, che in qualche modo ne incasellino i fenomeni e sfilaccino un po' il tessuto dei testi. L'esigenza di creare astrazioni formali rigorose √® stata uno stimolo importante per importanti studi teorici di sintassi e semantica, che hanno pi√π volte giovato degli strumenti e dei metodi della logica e della matematica.

Queste rappresentazioni formali possono, per alcuni compiti specifici, essere utili ancora oggi. L'approccio dominante √® per√≤ un altro, ed √® quello solitamente indicato attraverso  l'espressione ***natural language processing*** **(NLP)**. Il corrispondente terminologico italiano, meno fortunato, √® elaborazione (o trattamento) automatico del linguaggio. Come locuzione, NLP pone l'accento sul *processing* del linguaggio compiuto dal computer, sul fatto cio√® che **il computer viene addestrato a elaborare, trattare, manipolare testi in linguaggio naturale col fine di estrarne informazioni, di trasformarli** (come nel caso della traduzione automatica) **o di produrne di nuovi** (come nel caso della generazione automatica di riassunti, o di agenti che intrattengono conversazioni con umani). L'addestramento avviene attraverso i metodi statistici e probabilistici usati comunemente nel *machine learning* (vedi anche dopo) e nell'immenso ambito della *data science* per risolvere problemi di natura varia (anche problemi che non hanno nulla a che fare col linguaggio). Possiamo quindi vedere l'NLP come un'espressione che sottolinea le dimensioni "pi√π dure" (matematiche, informatiche, tecniche) dell'ambito, le quali in effetti oggi sono di estrema importanza. Similmente, ***(human) language technology*** (in italiano tecnologie del linguaggio) pone l'accento sull'insieme di tecnologie e tipi di dispositivi utilizzati per l'addestramento e per l'integrazione di IA dotate di competenza linguistica (ad esempio assistenti vocali e agenti conversazionali).

A volte si usa anche l'espressione ***natural language understanding*** **(NLU)**, la quale pone l'accento sul fatto che queste IA vengono addestrate, come gi√† detto, a comprendere i testi, a estrarne cio√® informazioni rilevanti. Estremamente correlate sono anche le espressioni ***text mining*** e ***text analytics***, solitamente usate intercambiabilmente (ed entrambe non tradotte in italiano). Sottolineano che il computer impara a compiere un processo di estrazione dell'informazione a partire dai testi, i quali sono potenzialmente *miniere* di informazioni utili. Queste informazioni sono per√≤ trasmesse in maniera *non strutturata*, ovvero sono trasmesse in una maniera sempre diversa in base allo stile e alle esigenze di chi scrive (torniamo esattamente al problema testo-tessuto di cui prima!). Prendiamo l'esempio delle recensioni Amazon: **per un computer che non ha conoscenza del linguaggio, sarebbe difficile intuire che "lo schermo √® di ottima qualit√†" e "trovo buono il touch screen" siano frasi emotivamente equivalenti, contenenti un po' la stessa informazione**. Una volta dotato il computer di competenza linguistica, per√≤, dati di questo tipo sono ricavabili in maniera automatica, e un rivenditore pu√≤ avere velocemente una panoramica di ci√≤ che gli utenti pensano dei suoi prodotti (ad esempio: *quali caratteristiche del prodotto sono particolarmente apprezzate, e quali sono invece da rivedere?*), il che pu√≤ essere ovviamente molto utile per prendere decisioni di sviluppo future.

Come forse √® gi√† evidente, poi, la linguistica computazionale si struttura in sotto-aree che si concentrano su aspetti specifici del linguaggio. La ***conversational AI*** (intelligenza artificiale conversazionale), ad esempio, si concentra sulla creazione dei gi√† citati agenti conversazionali, con l'obiettivo di dotarli di capacit√† linguistiche e pragmatiche quanto pi√π simili a quelle umane. La ***(automatic) speech recognition*** (riconoscimento automatico del parlato) invece sviluppa sistemi in grado di comprendere in maniera sempre pi√π precisa ed estesa i testi prodotti oralmente.

Infine, dal punto di vista terminologico vale la pena soffermarsi su una locuzione che √® stata usata poco su, cio√® ***machine learning***. Il machine learning √® una branca dell'IA (a oggi, la *principale* branca dell'IA), e si occupa di creare sistemi che imparino autonomamente, attraverso l'esposizione ai dati (in italiano infatti si parla di apprendimento automatico). Ci√≤ vuol dire che la "conoscenza" che questi sistemi acquisiscono non √® definita *a priori* da umani, attraverso delle regole, ma √® una conoscenza probabilistico-statistica che i sistemi definiscono attraverso l'apprendimento. Il ***deep learning*** (in italiano, apprendimento profondo) √®, a sua volta, una branca del *machine learning*): √® un termine utilizzato per indicare quelle IA che usano un particolare tipo di tecnologia, cio√® le reti neurali composte da molti strati. Ispirate, dal punto di vista funzionale, dal modo in cui si trasmette l'informazione nel cervello biologico, le reti neurali rappresentano lo stato dell'arte in numerosissimi compiti, ivi inclusi compiti di linguistica computazionale.
#In particolare, i dati vengono prima di tutto "decomposti" in delle caratteristiche (*feature*), e il sistema impara a definire il "contributo"



**Ricapitolando, la linguistica computazionale**:
* utilizza il computer per fare luce su fenomeni linguistici nuovi, per analizzare dati testuali in maniera efficiente, e per sviluppare nuove teorie linguistiche;
* addestra modelli di IA attraverso tecniche statistiche e matematiche, affinch√© possano autonomamente lavorare col linguaggio simulando l'uso che ne facciamo noi parlanti;
* in particolare, spesso addestra IA che siano in grado di estrarre in maniera ordinata e sistematica dati utili a partire dai testi, i quali sono potenzialmente molto disordinati e variegati.

Proprio perch√© variegati sono gli obiettivi della linguistica computazionale, le tecniche e gli algoritmi che essa usa sono numerosi, e in costante espansione. In particolare, gli ultimissimi anni sono stati un momento di grandissima crescita per l'ambito, grazie alla messa a punto di una serie di nuovi algoritmi (su tutti, l'architettura Transformer) in grado di dotare il computer di un grado di conoscenza linguistica fino a qualche anno fa difficilmente immaginabile. Tali sviluppi continuano a ispirare la ricerca, che settimanalmente (per non dire quotidianamente) fa notevoli passi in avanti verso la creazione di un'intelligenza che, per quanto artificiale, sia anche il pi√π possibile umana.
